// extractors/scraper.js
import "dotenv/config";
import { getCompanies, ingestJobs } from "../supabase.js";
import { scrapeCompany } from "./router.js";

const BATCH_SIZE = Number(process.env.INGEST_BATCH_SIZE || 200);

function chunk(arr, size) {
  const out = [];
  for (let i = 0; i < arr.length; i += size) out.push(arr.slice(i, i + size));
  return out;
}

async function run() {
  console.log("ðŸš€ Starting job scraper...");

  const companies = await getCompanies();
  console.log(`ðŸ“¦ Companies fetched: ${companies.length}`);

  let totalScraped = 0;

  for (const c of companies) {
    const name = c?.name || "Unknown";
    try {
      console.log(`ðŸ”Ž Scraping ${name}`);

      const jobs = await scrapeCompany(c);

      if (!Array.isArray(jobs) || jobs.length === 0) {
        console.log(`âš ï¸ ${name}: 0 jobs`);
        continue;
      }

      totalScraped += jobs.length;

      // ingest in batches (ingest-jobs already dedupes inside)
      for (const b of chunk(jobs, BATCH_SIZE)) {
        await ingestJobs(b);
      }

      console.log(`âœ… ${name}: ${jobs.length} jobs`);
    } catch (e) {
      console.log(`âŒ ${name} failed: ${String(e?.message || e)}`);
    }
  }

  console.log(`âœ… TOTAL jobs scraped: ${totalScraped}`);
}

run().catch((e) => {
  console.error("ðŸ’¥ Scraper crashed:", e);
  process.exit(1);
});
