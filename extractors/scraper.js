// extractors/scraper.js
import { sendJobs } from "../supabase.js";
import scrapeGreenhouse from "./greenhouse.js";
import scrapeAshby from "./ashby.js";
import scrapeGeneric from "./scrapeGeneric.js";

async function run() {
  console.log("ğŸš€ Starting scraper...");

  const companies = [
    { name: "Stripe", careers_url: "https://stripe.com/jobs", ats: "greenhouse" },
    { name: "Zoom", careers_url: "https://careers.zoom.us", ats: "generic" },
    { name: "Uber", careers_url: "https://www.uber.com/us/en/careers/", ats: "generic" },
  ];

  let allJobs = [];

  for (const company of companies) {
    console.log(`ğŸ” Scraping ${company.name}`);

    let jobs = [];

    if (company.ats === "greenhouse") {
      jobs = await scrapeGreenhouse(company);
    } else if (company.ats === "ashby") {
      jobs = await scrapeAshby(company);
    } else {
      jobs = await scrapeGeneric(company);
    }

    allJobs.push(...jobs);
  }

  console.log(`ğŸ“¦ TOTAL jobs scraped: ${allJobs.length}`);

  if (allJobs.length === 0) {
    console.log("âš ï¸ No jobs found, skipping ingestion");
    return;
  }

  await sendJobs(allJobs);

  console.log("ğŸ‰ Scrape completed successfully");
}

run().catch((err) => {
  console.error("ğŸ’¥ Scraper crashed:", err);
  process.exit(1);
});
