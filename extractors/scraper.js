import fs from "fs";
import path from "path";
import { fileURLToPath } from "url";
import router from "./router.js";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const COMPANIES_FILE = path.join(__dirname, "../companies.json");

console.log("üöÄ Starting scraper...");

if (!fs.existsSync(COMPANIES_FILE)) {
  console.warn("‚ö†Ô∏è companies.json not found. Run discover first.");
  process.exit(0);
}

const companies = JSON.parse(fs.readFileSync(COMPANIES_FILE, "utf-8"));

if (!Array.isArray(companies) || companies.length === 0) {
  console.warn("‚ö†Ô∏è No companies to scrape");
  process.exit(0);
}

let allJobs = [];

for (const company of companies) {
  console.log(`üîé Scraping ${company.name}`);
  try {
    const jobs = await router(company);
    console.log(`‚û°Ô∏è Found ${jobs.length} jobs`);
    allJobs.push(...jobs);
  } catch (err) {
    console.error(`‚ùå Failed ${company.name}`, err.message);
  }
}

console.log(`üì¶ TOTAL jobs scraped: ${allJobs.length}`);
console.log("üéâ Scrape completed successfully");
