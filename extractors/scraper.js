// extractors/scraper.js
import { sendJobs } from "../supabase.js";

async function scrapeGreenhouse(slug, company) {
  const url = `https://boards-api.greenhouse.io/v1/boards/${slug}/jobs`;

  let res;
  try {
    res = await fetch(url, { headers: { "User-Agent": "job-scraper/1.0" } });
  } catch (e) {
    console.error(`âŒ Network error for ${company}`);
    return [];
  }

  if (!res.ok) {
    console.warn(`âš ï¸ ${company} returned ${res.status}`);
    return [];
  }

  let data;
  try {
    data = await res.json();
  } catch {
    console.warn(`âš ï¸ Invalid JSON for ${company}`);
    return [];
  }

  // âœ… Support both Greenhouse response formats
  const jobsArray =
    Array.isArray(data.jobs)
      ? data.jobs
      : Array.isArray(data.data)
      ? data.data
      : [];

  if (!jobsArray.length) {
    console.warn(`âš ï¸ No jobs found for ${company}`);
    return [];
  }

  return jobsArray.map(j => ({
    title: j.title || "Unknown role",
    company,
    location: j.location?.name || "US",
    url: j.absolute_url || j.url,
    ats_source: "greenhouse",
    posted_at: j.updated_at || new Date().toISOString()
  }));
}

console.log("ğŸš€ Starting scraper...");

const targets = [
  { company: "Stripe", slug: "stripe" },
  { company: "Uber", slug: "uber" },
  { company: "Zoom", slug: "zoom" }
];

let allJobs = [];

for (const t of targets) {
  console.log(`ğŸ” Scraping ${t.company}`);
  const jobs = await scrapeGreenhouse(t.slug, t.company);
  console.log(`â¡ï¸ Found ${jobs.length} jobs`);
  allJobs.push(...jobs);
}

console.log(`ğŸ“¦ TOTAL jobs scraped: ${allJobs.length}`);

if (allJobs.length === 0) {
  console.warn("âš ï¸ No jobs scraped â€” skipping ingestion");
  process.exit(0);
}

await sendJobs(allJobs);

console.log("ğŸ‰ Scrape completed successfully");
